---
networks:
  ollama-network:
    # external: true # only comment out if you want others in your business
    name: ollama-network
volumes:
  ollama:
    driver: local
    # external: true
  open-webui:
    driver: local
services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    pull_policy: always
    networks:
      - ollama-network
    container_name: open-webui
    ports:
      - 3000:8080
    # environment:
    #   - OLLAMA_BASE_URL=NETWORK_ADDRESS:11434 # for others to consume
    volumes:
      - open-webui:/app/backend/data
    restart: unless-stopped
  ollama:
    image: ollama/ollama
    pull_policy: always
    networks:
      - ollama-network
    container_name: ollama
    # mem_limit: 2g  # Limit container memory usage to 2 GiB
    # mem_reservation: 1g  # Reserve 1 GiB of memory for the container
    ports:
      - 11434:11434
    volumes:
      - ollama:/root/.ollama
    command:
      - serve
    deploy:
      resources:
        limits:
          memory: 64g
        # reservations:
        #   devices:
        #     - driver: nvidia
        #       count: all
        #       capabilities: [gpu]
    restart: unless-stopped